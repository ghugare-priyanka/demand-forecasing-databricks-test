{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fde3784-d8da-4eb6-8710-e546e3b02f9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install \"flaml[automl]\" openml\n",
    "\n",
    "#dbutils.library.restartPython() #to reflect the flaml lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7301d33-e3ac-4159-8a2a-3d5de1b69309",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from itertools import product\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "from flaml import AutoML\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import boto3\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce1768a-7a6f-42de-9d39-848810ee121a",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data_from_unity_catalog():\n",
    "    # Data via unity catalog\n",
    "    # Set the current catalog and schema (database) if necessary\n",
    "    spark.sql(\"USE CATALOG `edp-apac-uat`\") # Unity Catalog name\n",
    "    spark.sql(\"USE l1_asurion_apac\") # Schema (database) name\n",
    "\n",
    "    # Query a table\n",
    "    preprocessed_data = spark.sql(\"SELECT * FROM preprocessed_data_15feb2024\") # Table name #spark.read.table\n",
    "    #preprocessed_data.show()\n",
    "\n",
    "    # Print the schema to understand data types\n",
    "    # preprocessed_data.printSchema()\n",
    "    preprocessed_data.display()\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6b37e4-8dac-4239-9b68-8868de67be89",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data = read_data_from_unity_catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a135ce0-7b43-4057-b413-4129a3e4626b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preprocessed_data.isnull().sum()\n",
    "#print(preprocessed_data.isna())  \n",
    "#type(preprocessed_data)\n",
    "\n",
    "preprocessed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6c384c4-f585-4eb2-a10f-63980f20f9b5",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03fcd03b-6940-4f61-8113-e14f00f3cbc1",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Predecessor'].fillna('None', inplace=True)\n",
    "preprocessed_data['Successor'].fillna('None', inplace=True)\n",
    "preprocessed_data['Model_Series'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cbc5da2-93db-4979-8008-2608976bcce6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400aa467-79c2-467d-b3d6-2f4dd16ecf14",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f49cb1f-5f02-467f-8532-eaed723a73e6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Product_Launch_Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81693d53-dbe7-423f-9e89-3c37f78d9ef3",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Model_Series_Launch_Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f644d2be-fcc4-4057-bb5c-6b2401c5da92",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "def parse_mixed_dates(date_str): # Define the date formats to try \n",
    "    date_formats = ['%Y-%m-%d', '%d-%m-%Y'] \n",
    "\n",
    "    # Iterate over the date formats and try to convert the date string to a datetime object \n",
    "    for fmt in date_formats: \n",
    "        try: \n",
    "            return pd.to_datetime(date_str, format=fmt) \n",
    "        except ValueError: \n",
    "            continue \n",
    "    # If none of the formats work, return pd.NaT \n",
    "    return pd.NaT # Assuming 'df' is your DataFrame and 'mixed_date_col' is the column with mixed date formats\n",
    "\n",
    "preprocessed_data['Product_Launch_Date'] = preprocessed_data['Product_Launch_Date'].apply(parse_mixed_dates)\n",
    "# Verify the result \n",
    "print(preprocessed_data['Product_Launch_Date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "691b143b-997a-4261-9ab9-7e32766415d4",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Model_Series_Launch_Date'] = preprocessed_data['Model_Series_Launch_Date'].apply(parse_mixed_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c572b7c-0906-45bb-8ed3-ce4f455efddc",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(preprocessed_data['Model_Series_Launch_Date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c953a917-c480-442b-8d25-11f0d77a45a0",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Product_Launch_Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49407853-abb6-4771-922b-f5850781cc5b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Model_Series_Launch_Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "694a34c4-e953-4496-b7fe-4fcc7ae337c8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0435c69-de13-4815-9f1e-61d290d1df20",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract year and month into new columns for 'Product_Launch_Date' \n",
    "preprocessed_data['Product_Launch_Year'] = preprocessed_data['Product_Launch_Date'].dt.year \n",
    "preprocessed_data['Product_Launch_Month'] = preprocessed_data['Product_Launch_Date'].dt.month \n",
    "# Extract year and month into new columns for 'Model_Series_Launch_Date' \n",
    "preprocessed_data['Model_Series_Launch_Year'] = preprocessed_data['Model_Series_Launch_Date'].dt.year \n",
    "preprocessed_data['Model_Series_Launch_Month'] = preprocessed_data['Model_Series_Launch_Date'].dt.month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac3589a8-35de-488d-954f-c3e1b7ee6093",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c989bd8a-990f-45d0-afdb-c5fff0d0d771",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3a47d88-c683-4c44-a391-23164652a4a4",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make Model Type columns\n",
    "type_patterns = ['PLUS', 'ULTRA', 'FOLD', 'EDGE', 'DUO', 'FLIP', 'FE','STAR','LITE','PRO MAX', 'PRO', 'MINI', 'MAX']\n",
    "\n",
    "# Extract MODEL_TYPE based on patterns\n",
    "preprocessed_data['Model_Type'] = preprocessed_data['Model_Family'].str.extract('(' + '|'.join(type_patterns) + ')', expand=False)\n",
    "\n",
    "# Fill missing values in MODEL_TYPE with 'Regular'\n",
    "preprocessed_data['Model_Type'].fillna('BASIC', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f89aadb-9f36-40b3-83e3-a6b9de6d51e6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c6a0322-bded-4767-8707-30d0beda93a9",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "\n",
    "\n",
    "def determine_optimal_lags(df, target_column, series_column, pacf_threshold=0.2, max_lags_limit=40):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"The DataFrame is empty.\")\n",
    "    if target_column not in df.columns or series_column not in df.columns:\n",
    "        raise ValueError(\"Target or series column not found in DataFrame.\")\n",
    "\n",
    "    optimal_lags = {}\n",
    "    for series in df[series_column].unique():\n",
    "        series_data = df[df[series_column] == series][target_column].dropna()\n",
    "        if len(series_data) < 2:\n",
    "            continue  # Skip series with too few data points\n",
    "\n",
    "        # Adjusting max_lags to be within the allowed limit\n",
    "        max_allowed_lags = len(series_data) // 2 - 1\n",
    "        max_lags = min(max_lags_limit, max_allowed_lags)\n",
    "        \n",
    "        if max_lags < 1:\n",
    "            continue\n",
    "\n",
    "        pacf_vals = pacf(series_data, nlags=max_lags, method='ols')\n",
    "        significant_lags = [i for i, val in enumerate(pacf_vals) if abs(val) > pacf_threshold]\n",
    "        \n",
    "        if significant_lags:\n",
    "            optimal_lags[series] = max(significant_lags)\n",
    "        else:\n",
    "            optimal_lags[series] = 0  # No significant lags found\n",
    "\n",
    "    return optimal_lags\n",
    "\n",
    "\n",
    "\n",
    "def create_common_optimal_lagged_features(df, target_column, series_column, optimal_lags):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"The DataFrame is empty.\")\n",
    "    if target_column not in df.columns or series_column not in df.columns:\n",
    "        raise ValueError(\"Target or series column not found in DataFrame.\")\n",
    "\n",
    "    # Determine a common optimal number of lags (e.g., median or maximum)\n",
    "    common_optimal_lag = np.median(list(optimal_lags.values()))\n",
    "\n",
    "    lagged_df = pd.DataFrame()\n",
    "    for series in df[series_column].unique():\n",
    "        series_data = df[df[series_column] == series].copy()\n",
    "\n",
    "        for lag in range(1, int(common_optimal_lag) + 1):\n",
    "            series_data[f'lag_{lag}'] = series_data[target_column].shift(lag)\n",
    "            \n",
    "        # Handle missing values in newly created features\n",
    "        #series_data.dropna(inplace=True)\n",
    "\n",
    "        # First interpolate, then forward fill\n",
    "        series_data.interpolate(method='linear', inplace=True)\n",
    "        series_data.fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        lagged_df = pd.concat([lagged_df, series_data], ignore_index=True)\n",
    "\n",
    "    return lagged_df\n",
    "    \n",
    "\n",
    "series_column = 'Model'\n",
    "target_column = 'Shipped_Claim'\n",
    "optimal_lags = determine_optimal_lags(preprocessed_data, target_column, series_column)\n",
    "lagged_df = create_common_optimal_lagged_features(preprocessed_data, target_column, series_column, optimal_lags)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "466f275c-7eb1-4363-befa-4553e1b2ee79",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.sort_values(by='YearMonth', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "365850ac-0d84-41b1-8542-b208afb9e407",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from statsmodels.tsa.stattools import pacf \n",
    "from statsmodels.graphics.tsaplots import plot_pacf \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has a column 'y' which is the time series data \n",
    "# Calculate PACF with confidence intervals \n",
    "lags = 40 # Define the number of lags you want to test \n",
    "alpha = 0.05 # Significance level for the confidence intervals \n",
    "\n",
    "pacf_values, confint = pacf(preprocessed_data['Shipped_Claim'], nlags=lags, alpha=alpha) \n",
    "\n",
    "# Plot PACF with confidence intervals \n",
    "plot_pacf(preprocessed_data['Shipped_Claim'], lags=lags, alpha=alpha) \n",
    "plt.show() \n",
    "\n",
    "# Automatically identify the optimal lag \n",
    "# The optimal lag is considered to be the last significant lag, \n",
    "# i.e., the last lag before the PACF falls within the confidence interval bounds for the first time. \n",
    "optimal_lag = 0 \n",
    "for i in range(1, len(pacf_values)): \n",
    "    if pacf_values[i] < confint[i][0] or pacf_values[i] > confint[i][1]: \n",
    "        optimal_lag = i \n",
    "    else: break # Stop at the first non-significant lag \n",
    "print(f\"Optimal lag: {optimal_lag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdef37f6-d597-4c62-8f44-cd31bf816aef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"# Create lag columns based on the optimal lag \n",
    "for lag in range(1, optimal_lag + 1): \n",
    "    preprocessed_data[f'Lag_{lag}'] = preprocessed_data['Shipped_Claim'].shift(lag) \n",
    "# Display the first few rows to verify the new columns \n",
    "print(df.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d6c5ff-de6e-4269-a4c5-1a43f607ede6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data['Shipped_Claim'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41a00cec-a98b-471e-b164-b8808c02f2af",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create lagged features\n",
    "number_of_lags = 3\n",
    "for lag in range(1, number_of_lags + 1):\n",
    "    preprocessed_data[f'Lag_{lag}'] = preprocessed_data['Shipped_Claim'].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08af64e5-8b19-4ffd-a0c6-adf212ea3b68",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72c803c-d390-4db0-a0a7-f4d7b29b0f5d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf32fb8-262f-4c7c-a6c3-9568a201524d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lag_cols= ['Lag_1', 'Lag_2', 'Lag_3']\n",
    "preprocessed_data[lag_cols] = preprocessed_data[lag_cols].apply(lambda x: x.fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09a64c99-38ef-4535-a294-b822d5fa4504",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "461a02ba-c5da-4179-8149-2345dd060e95",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98a387b9-a3f6-4198-953d-1ba94f211135",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datetime_cols = [ 'YearMonth'] \n",
    "\n",
    "cat_cols = ['Country', 'Client', 'Product', 'Program', 'Make', 'Model_Series', 'Model_Family', 'Predecessor', 'Model_Type',\n",
    "            'Successor','Model', 'Model_No_Color']\n",
    "\n",
    "num_cols = [ 'Model_Age_Days', 'Closing_Base', 'Model_Capacity', 'Year', 'Month', 'Product_Launch_Year', \n",
    "           'Product_Launch_Month','Model_Series_Launch_Year', 'Model_Series_Launch_Month', 'Lag_1', 'Lag_2', 'Lag_3']\n",
    "\n",
    "target_col = ['Shipped_Claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "441ef8bb-ed37-441c-b034-4dbb42efd4be",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoding for categorical variables\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    preprocessed_data[col] = label_encoders[col].fit_transform(preprocessed_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a8fb493-6cdb-4391-85d8-55738b995641",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e28954e-a4e2-4d70-a975-95508f8a6c54",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e29bc819-1418-44d7-b526-e690fe793be1",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data.sort_values(by='YearMonth', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59dc69ac-6d64-4e89-9624-b11fd3fdc96b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = preprocessed_data[ datetime_cols + cat_cols + num_cols + target_col ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f14adb9-03cd-4852-8834-2f1ddcdb8622",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ff8922-970e-432f-b334-5e98c96c1899",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26b76958-b784-4b79-95ce-3c998e870776",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ff22f1-92ee-4461-b5c0-3cd7fbad08ed",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eca66ab-6c78-4d99-9e1e-0d55b3c0634e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.frame import H2OFrame\n",
    "\n",
    "# Initialize H2O\n",
    "h2o.init()\n",
    "\n",
    "# Sort the DataFrame by the time column\n",
    "#final_df = final_df.sort_values('YearMonth')\n",
    "\n",
    "# Calculate the split point (80% training, 20% testing)\n",
    "split_point = int(len(final_df) * 0.875)\n",
    "\n",
    "# Split the data\n",
    "train_df = final_df.iloc[:split_point]\n",
    "test_df = final_df.iloc[split_point:]\n",
    "\n",
    "# Reset index in both training and test DataFrames\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Convert pandas DataFrames to H2OFrames\n",
    "train = h2o.H2OFrame(train_df)\n",
    "test = h2o.H2OFrame(test_df)\n",
    "\n",
    "# Define the target and features\n",
    "target = 'Shipped_Claim'\n",
    "features = [col for col in train.columns if col != target]\n",
    "\n",
    "# Set up H2O AutoML\n",
    "automl = H2OAutoML(max_runtime_secs=1200, max_models=20, seed=1, stopping_metric='RMSE', sort_metric='RMSE')\n",
    "\n",
    "# Train models\n",
    "automl.train(x=features, y=target, training_frame=train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = automl.leaderboard\n",
    "print(lb)\n",
    "\n",
    "# Get the best model\n",
    "best_model = automl.leader\n",
    "\n",
    "# Model performance on the training set\n",
    "train_performance = best_model.model_performance(train)\n",
    "print(\"\\nTraining Performance\")\n",
    "print(\"RMSE:\", train_performance.rmse())\n",
    "print(\"MAE:\", train_performance.mae())\n",
    "print(\"R2:\", train_performance.r2())\n",
    "\n",
    "# Model performance on the test set\n",
    "test_performance = best_model.model_performance(test)\n",
    "print(\"\\nTest Performance\")\n",
    "print(\"RMSE:\", test_performance.rmse())\n",
    "print(\"MAE:\", test_performance.mae())\n",
    "print(\"R2:\", test_performance.r2())\n",
    "\n",
    "# Example: Shut down H2O - Uncomment the below line when you are done with H2O\n",
    "# h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ac018b-7e33-43ca-b289-2f347520a1e8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime \n",
    "def split_dataset(df, test_months): \n",
    "    # Convert year and month to a datetime to ease sorting and manipulation\n",
    "    df['YearMonth'] = pd.to_datetime(df['YearMonth'])\n",
    "    df = df.sort_values(by='YearMonth') # Ensure the DataFrame is sorted by date \n",
    "    # Find the split date by subtracting test_months from the max date \n",
    "    max_date = df['YearMonth'].max() \n",
    "    split_date = max_date - pd.DateOffset(months=test_months) \n",
    "    # Split the dataset \n",
    "    train_df = df[df['YearMonth'] <= split_date] \n",
    "    test_df = df[df['YearMonth'] > split_date] \n",
    "    # Drop the date column if no longer needed \n",
    "    train_df = train_df.drop(columns=['YearMonth']) \n",
    "    test_df = test_df.drop(columns=['YearMonth']) \n",
    "    \n",
    "    return train_df, test_df \n",
    "\n",
    "\n",
    "test_months = 6 #  reserving the last n months of data for testing \n",
    "train, test = split_dataset(final_df, test_months) \n",
    "\n",
    "# AutoML setup\n",
    "# Define settings dictionary\n",
    "settings = { \n",
    "    \"time_budget\": 1200,\n",
    "    \"metric\": \"r2\", \n",
    "    \"estimator_list\": ['xgboost'], \n",
    "    \"task\": 'regression', \n",
    "    \"seed\": 42, } \n",
    "\n",
    "# Initialize AutoML \n",
    "automl = AutoML() \n",
    "\n",
    "# Train the model \n",
    "automl.fit(X_train=train.drop('Shipped_Claim', axis=1), y_train=train['Shipped_Claim'], **settings) \n",
    "# Predictions \n",
    "preds = automl.predict(test.drop('Shipped_Claim', axis=1)) \n",
    "# Print the results \n",
    "print('Best hyperparameter config:', automl.best_config) \n",
    "print('Best r2 on validation data: {:.4g}'.format(1 - automl.best_loss)) \n",
    "print('Training duration of best run: {:.4g} s'.format(automl.best_config_train_time)) \n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0fd19e-553e-4d2e-b218-d7a3b89d10e6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "# Calculate RMSE, R2, and MAE for training set \n",
    "train_true = train['Shipped_Claim'] \n",
    "train_preds = automl.predict(train.drop('Shipped_Claim', axis=1)) \n",
    "train_rmse = metrics.mean_squared_error(train_true, train_preds, squared=False) \n",
    "train_r2 = metrics.r2_score(train_true, train_preds) \n",
    "train_mae = metrics.mean_absolute_error(train_true, train_preds) \n",
    "# Calculate RMSE, R2, and MAE for test set \n",
    "test_true = test['Shipped_Claim'] \n",
    "test_preds = preds \n",
    "# assuming 'preds' contains your test set predictions from the previous step \n",
    "test_rmse = metrics.mean_squared_error(test_true, test_preds, squared=False) \n",
    "test_r2 = metrics.r2_score(test_true, test_preds) \n",
    "test_mae = metrics.mean_absolute_error(test_true, test_preds) \n",
    "# Print the metrics \n",
    "print(f\"Training RMSE: {train_rmse}\") \n",
    "print(f\"Training R^2: {train_r2}\") \n",
    "print(f\"Training MAE: {train_mae}\\n\") \n",
    "print(f\"Test RMSE: {test_rmse}\") \n",
    "print(f\"Test R^2: {test_r2}\") \n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf51c82b-bc7e-4fcc-b191-8d2b409e5309",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime \n",
    "def split_dataset(df, test_months): \n",
    "    # Convert year and month to a datetime to ease sorting and manipulation\n",
    "    df['YearMonth'] = pd.to_datetime(df['YearMonth'])\n",
    "    df = df.sort_values(by='YearMonth') # Ensure the DataFrame is sorted by date \n",
    "    # Find the split date by subtracting test_months from the max date \n",
    "    max_date = df['YearMonth'].max() \n",
    "    split_date = max_date - pd.DateOffset(months=test_months) \n",
    "    # Split the dataset \n",
    "    train_df = df[df['YearMonth'] <= split_date] \n",
    "    test_df = df[df['YearMonth'] > split_date] \n",
    "    # Drop the date column if no longer needed \n",
    "    train_df = train_df.drop(columns=['YearMonth']) \n",
    "    test_df = test_df.drop(columns=['YearMonth']) \n",
    "    \n",
    "    return train_df, test_df \n",
    "\n",
    "\n",
    "test_months = 6 #  reserving the last n months of data for testing \n",
    "train, test = split_dataset(final_df, test_months) \n",
    "\n",
    "# AutoML setup\n",
    "# Define settings dictionary\n",
    "settings = { \n",
    "    \"time_budget\": 1200,\n",
    "    \"metric\": \"r2\", \n",
    "    \"estimator_list\": ['xgboost', 'lgbm', 'xgb_limitdepth'], \n",
    "    \"task\": 'regression', \n",
    "    \"seed\": 42, } \n",
    "\n",
    "# Initialize AutoML \n",
    "automl = AutoML() \n",
    "\n",
    "# Train the model \n",
    "automl.fit(X_train=train.drop('Shipped_Claim', axis=1), y_train=train['Shipped_Claim'], **settings) \n",
    "# Predictions \n",
    "preds = automl.predict(test.drop('Shipped_Claim', axis=1)) \n",
    "# Print the results \n",
    "print('Best hyperparameter config:', automl.best_config) \n",
    "print('Best r2 on validation data: {:.4g}'.format(1 - automl.best_loss)) \n",
    "print('Training duration of best run: {:.4g} s'.format(automl.best_config_train_time)) \n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc89ac9-d100-4aa7-aca9-ac249af682a7",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "# Calculate RMSE, R2, and MAE for training set \n",
    "train_true = train['Shipped_Claim'] \n",
    "train_preds = automl.predict(train.drop('Shipped_Claim', axis=1)) \n",
    "train_rmse = metrics.mean_squared_error(train_true, train_preds, squared=False) \n",
    "train_r2 = metrics.r2_score(train_true, train_preds) \n",
    "train_mae = metrics.mean_absolute_error(train_true, train_preds) \n",
    "# Calculate RMSE, R2, and MAE for test set \n",
    "test_true = test['Shipped_Claim'] \n",
    "test_preds = preds \n",
    "# assuming 'preds' contains your test set predictions from the previous step \n",
    "test_rmse = metrics.mean_squared_error(test_true, test_preds, squared=False) \n",
    "test_r2 = metrics.r2_score(test_true, test_preds) \n",
    "test_mae = metrics.mean_absolute_error(test_true, test_preds) \n",
    "# Print the metrics \n",
    "print(f\"Training RMSE: {train_rmse}\") \n",
    "print(f\"Training R^2: {train_r2}\") \n",
    "print(f\"Training MAE: {train_mae}\\n\") \n",
    "print(f\"Test RMSE: {test_rmse}\") \n",
    "print(f\"Test R^2: {test_r2}\") \n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212dff9d-360b-40a7-868e-fa26ee127603",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['forecast'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffe05353-5ea2-4aa0-8c76-2d78c1e4886d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd60ee01-6b2c-4f15-b3c3-9ca7c6294f76",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706e9c41-4eb2-430c-971c-dece18cfbce6",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    predictions[col] = label_encoders[col].inverse_transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2243966d-2786-4e46-afc1-9ea3e2f2d4fc",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1adce9e4-9a8c-453a-b738-777464d13c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "XGB_15Feb_ClientMonthlyData",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
